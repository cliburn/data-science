Data Science for Biostatisticians
----

1. How to use the command line
    2. Basic Unix commands
    3. Chaining commands
    4. I/O
    5. Piping and redirection
    6. Using a text editor
7. Installing software via a package manager
7. Installing software from source
1. How to obtain data
2. From text files (CSV, TSV)
2. From unstructured text
3. From JSON files
4. From XML files
3. From Excel files
4. From a web (REST) API
4. From scraping the web
4. From a relational database
5. From a NoSQL database
6. From a SPARQL database
7. How to clean data
8. Cleaning text data with string functions
9. Cleaning text with regular expressions
9. Setting validation rules
10. Mapping to standard vocabularies
2. How to work with DataFrames
3. Anatomy of a DataFrame
4. Indexing and extracting subsets
5. Sorting
6. Tall to Wide
7. Wide to Tall
10. The split-apply-combine pattern
8. Grouping
9. Function application
10. Concatenating DataFrames
11. Merging and joining DataFrames
10. Saving and exporting DataFrames
3. How to explore data
4. Summary statistics
5. Constructing meaningful tables
5. Introduction to statistical visualization
6. Working with domain experts
11. How to work with missing data and outliers
12. Identifying outliers
13. Types of missing data
14. Simple methods
15. Single imputation
16. Multiple imputation
4. How to model data
5. Review of probability models
6. Review of data transformations
5. Review of linear models
6. Review of Generalized linear models
8. Multi-level models
9. Fitting models to data
7. Fitting nonlinear models to data
10. Interpreting model fits
11. Checking model assumptions
12. Using simulations
13. Combining information from multiple sources
5. How to build a pipeline
6. Why build a pipeline
7. Chaining steps
8. Cross-validation and pipelines
5. How to display results
6. Native plotting libraries
7. Graphics for the web
8. Graphics for high-dimensional data
9. Making interactive plots
10. Plotting geographical data
11. Plotting graphical and network data
6. How to improve performance
7. Profiling and benchmarking code
7. Understanding algorithmic complexity
8. Choice of data structures and algorithms
9. Wrapping native code
10. Running jobs in parallel
7. How to make analysis reproducible
8. Literate programming
9. Version control
9. Automating tasks
10. Using VMs and Docker containers for reproducible environments
11. Testing code and test generators
12. Code coverage
11. Use of `make` to build projects
8. How to work with data too big for RAM
9. Buy more RAM
10. Change of algorithm/data structure
9. Lazy evaluation
10. Memory Mapping
11. Sub-sampling
12. Distributed computing
13. Case studies (interspersed throughout course)
14. Working with EHR data
15. Working with image data
16. Working with genomics data
17. Working with survey data
18. Working with Quantified Self data
